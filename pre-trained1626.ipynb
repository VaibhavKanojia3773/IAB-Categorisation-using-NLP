{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-06T10:56:16.777010Z","iopub.status.busy":"2024-10-06T10:56:16.776151Z","iopub.status.idle":"2024-10-06T10:56:29.446488Z","shell.execute_reply":"2024-10-06T10:56:29.445486Z","shell.execute_reply.started":"2024-10-06T10:56:16.776960Z"},"trusted":true},"outputs":[],"source":["# Check CPU information\n","import os\n","import psutil\n","\n","# Number of CPU cores\n","cpu_cores = psutil.cpu_count(logical=True)\n","print(f\"Number of CPU cores: {cpu_cores}\")\n","\n","# CPU Memory\n","cpu_memory = psutil.virtual_memory()\n","print(f\"Total CPU Memory: {cpu_memory.total / (1024**3):.2f} GB\")\n","print(f\"Available CPU Memory: {cpu_memory.available / (1024**3):.2f} GB\")\n","print(f\"Used CPU Memory: {cpu_memory.used / (1024**3):.2f} GB\")\n","print(f\"Memory Usage Percentage: {cpu_memory.percent}%\")\n","\n","# TPU information\n","# TPU is accessed through TensorFlow, so we'll check TPU usage\n","import tensorflow as tf\n","\n","# Check TPU\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print(\"TPU Available.\")\n","    print(f\"TPU Name: {tpu.master()}\")\n","\n","    # You can also get TPU information through TensorFlow\n","    tpu_details = tf.tpu.experimental.initialize_tpu_system(tpu)\n","    print(\"TPU initialized.\")\n","except ValueError as e:\n","    print(\"TPU not available.\")\n","\n","# If you want to check RAM specifically in a TPU environment, you can do:\n","if 'COLAB_GPU' in os.environ:\n","    print(\"Using GPU Runtime\")\n","elif 'COLAB_TPU_ADDR' in os.environ:\n","    print(\"Using TPU Runtime\")\n","else:\n","    print(\"Using CPU Runtime\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn import metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","\n","# Use your token directly\n","hf_token = \"token\"\n","login(token=hf_token)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 1: Load the pre-trained model and tokenizer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","model_name = \"PavanDeepak/text-classification-model-iab-categories-mixed-bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Move the model to the appropriate device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 2: Load the test data\n","test_data = pd.read_csv('/kaggle/input/trial-fibe1/dataset/test.csv', encoding='ISO-8859-1')  # Update with your actual path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 3: Tokenize the test data\n","test_encodings = tokenizer(\n","    test_data['text'].tolist(),\n","    truncation=True,\n","    padding=True,\n","    max_length=512,\n","    return_tensors='pt'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","batch_size = 32  # Adjust this based on your available memory\n","predictions = []\n","\n","# Process the input in batches\n","for i in range(0, len(test_encodings['input_ids']), batch_size):\n","    batch_input_ids = test_encodings['input_ids'][i:i + batch_size].to(device)\n","    batch_attention_mask = test_encodings['attention_mask'][i:i + batch_size].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n","        batch_predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n","        predictions.extend(batch_predictions)\n","\n","predictions = np.array(predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 5: Prepare the submission file\n","submission = pd.DataFrame({\n","    'Index': test_data['Index'],  # Ensure your test.csv has an 'Index' column\n","    'target': predictions\n","})\n","\n","submission.to_csv('submissions.csv', index=False)\n","print(\"Submission file created: submissions.csv\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5799469,"sourceId":9524289,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
