{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9524289,"sourceType":"datasetVersion","datasetId":5799469}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Check CPU information\nimport os\nimport psutil\n\n# Number of CPU cores\ncpu_cores = psutil.cpu_count(logical=True)\nprint(f\"Number of CPU cores: {cpu_cores}\")\n\n# CPU Memory\ncpu_memory = psutil.virtual_memory()\nprint(f\"Total CPU Memory: {cpu_memory.total / (1024**3):.2f} GB\")\nprint(f\"Available CPU Memory: {cpu_memory.available / (1024**3):.2f} GB\")\nprint(f\"Used CPU Memory: {cpu_memory.used / (1024**3):.2f} GB\")\nprint(f\"Memory Usage Percentage: {cpu_memory.percent}%\")\n\n# TPU information\n# TPU is accessed through TensorFlow, so we'll check TPU usage\nimport tensorflow as tf\n\n# Check TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"TPU Available.\")\n    print(f\"TPU Name: {tpu.master()}\")\n\n    # You can also get TPU information through TensorFlow\n    tpu_details = tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(\"TPU initialized.\")\nexcept ValueError as e:\n    print(\"TPU not available.\")\n\n# If you want to check RAM specifically in a TPU environment, you can do:\nif 'COLAB_GPU' in os.environ:\n    print(\"Using GPU Runtime\")\nelif 'COLAB_TPU_ADDR' in os.environ:\n    print(\"Using TPU Runtime\")\nelse:\n    print(\"Using CPU Runtime\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T10:56:16.776151Z","iopub.execute_input":"2024-10-06T10:56:16.777010Z","iopub.status.idle":"2024-10-06T10:56:29.446488Z","shell.execute_reply.started":"2024-10-06T10:56:16.776960Z","shell.execute_reply":"2024-10-06T10:56:29.445486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn import metrics\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Use your token directly\nhf_token = \"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\"\nlogin(token=hf_token)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Load the pre-trained model and tokenizer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"PavanDeepak/text-classification-model-iab-categories-mixed-bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move the model to the appropriate device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Load the test data\ntest_data = pd.read_csv('/kaggle/input/trial-fibe1/dataset/test.csv', encoding='ISO-8859-1')  # Update with your actual path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Tokenize the test data\ntest_encodings = tokenizer(\n    test_data['text'].tolist(),\n    truncation=True,\n    padding=True,\n    max_length=512,\n    return_tensors='pt'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nbatch_size = 32  # Adjust this based on your available memory\npredictions = []\n\n# Process the input in batches\nfor i in range(0, len(test_encodings['input_ids']), batch_size):\n    batch_input_ids = test_encodings['input_ids'][i:i + batch_size].to(device)\n    batch_attention_mask = test_encodings['attention_mask'][i:i + batch_size].to(device)\n\n    with torch.no_grad():\n        outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n        batch_predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        predictions.extend(batch_predictions)\n\npredictions = np.array(predictions)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 5: Prepare the submission file\nsubmission = pd.DataFrame({\n    'Index': test_data['Index'],  # Ensure your test.csv has an 'Index' column\n    'target': predictions\n})\n\nsubmission.to_csv('submissions.csv', index=False)\nprint(\"Submission file created: submissions.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}