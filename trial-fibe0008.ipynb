{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9524289,"sourceType":"datasetVersion","datasetId":5799469},{"sourceId":9557438,"sourceType":"datasetVersion","datasetId":5823846}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install Required Libraries\n!pip install transformers torch_xla\n\n# Step 2: Imports\nimport os\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn import metrics\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# Step 3: Load Data\ntrain_data = pd.read_csv('/kaggle/input/trial-fibe1/dataset/train.csv', encoding='ISO-8859-1')\ntest_data = pd.read_csv('/kaggle/input/trial-fibe1/dataset/test.csv', encoding='ISO-8859-1')\nX = train_data['text']\ny = train_data['target']\n\n# Step 4: Split Data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Step 5: Tokenization\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\ndef tokenize_and_clean(texts):\n    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n    # Clean up tokenization spaces\n    encodings['input_ids'] = [tokenizer.clean_up_tokenization_spaces(ids) for ids in encodings['input_ids']]\n    return encodings\n\ntrain_encodings = tokenize_and_clean(list(X_train))\nval_encodings = tokenize_and_clean(list(X_val))\n\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train)\ny_val_encoded = le.transform(y_val)\n\ntrain_inputs = torch.tensor(train_encodings['input_ids'], dtype=torch.long)\nval_inputs = torch.tensor(val_encodings['input_ids'], dtype=torch.long)\n\n# Step 6: Create Custom Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, inputs, labels):\n        self.inputs = inputs\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return self.inputs[idx], self.labels[idx]\n\ntrain_dataset = NewsDataset(train_inputs, y_train_encoded)\nval_dataset = NewsDataset(val_inputs, y_val_encoded)\n\n# Step 7: DataLoader Creation with multi-threading\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=12)  # Use multiple workers\nval_loader = DataLoader(val_dataset, batch_size=8, num_workers=12)\n\n# Step 8: Load Model\ndevice = xm.xla_device()\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(le.classes_)).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Step 9: Checkpoint Functions\ncheckpoint_dir = '/kaggle/working/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\ndef save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth'):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch,\n        'loss': loss,\n    }\n    torch.save(checkpoint, os.path.join(checkpoint_dir, filename))\n\n# Load the checkpoint if available (load on CPU first)\ncheckpoint_path = '/kaggle/input/epoch11/checkpoint_epoch_1 (1).pth'\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\nelse:\n    print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n    start_epoch = 0  # Start from epoch 0\n\nnum_epochs = 1  # Change this to the number of additional epochs you want to run\nscaler = GradScaler()\naccumulation_steps = 4  # Adjust as needed\n\n# Step 10: Training Loop\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    model.train()\n    total_loss = 0\n    for i, batch in enumerate(train_loader):\n        input_ids, labels = batch\n        input_ids = input_ids.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        \n        with autocast():\n            outputs = model(input_ids, labels=labels)\n            loss = outputs.loss / accumulation_steps\n        \n        total_loss += loss.item()\n        \n        # Backward pass\n        scaler.scale(loss).backward()\n\n        if (i + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            xm.mark_step()  # Ensure proper synchronization\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n    \n    # Save checkpoint every epoch\n    save_checkpoint(model, optimizer, epoch, total_loss / len(train_loader), filename=f'checkpoint_epoch_{epoch + 1}.pth')\n\n# Step 11: Model Evaluation\nmodel.eval()\ntotal_accuracy = 0\ntotal_samples = 0\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs, labels = batch\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model(inputs)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        total_accuracy += (predictions == labels).sum().item()\n        total_samples += labels.size(0)\n\nval_accuracy = total_accuracy / total_samples\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n\n# Step 12: Predictions on Test Data in smaller chunks to avoid memory issues\nbatch_size = 8  # You can adjust this as needed\ntest_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True, max_length=512)\ntest_inputs = torch.tensor(test_encodings['input_ids'], dtype=torch.long).to(device)\n\ntest_predictions = []\nwith torch.no_grad():\n    for i in range(0, len(test_inputs), batch_size):\n        batch = test_inputs[i:i + batch_size]\n        outputs = model(batch)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())  # Move to CPU and extend list\n\n# Step 13: Prepare Submission\nsubmission = pd.DataFrame({'Index': test_data['Index'], 'target': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\n\n# Step 14: Calculate and Print F1 Score\ny_val_pred = []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs, labels = batch\n        inputs = inputs.to(device)\n\n        outputs = model(inputs)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        y_val_pred.extend(predictions.cpu().numpy())\n\nscore = metrics.f1_score(y_val_encoded, y_val_pred, average='weighted')\nprint(f\"F1 Score: {score:.2f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}